{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4rCKcndPybL"
      },
      "source": [
        "# __Project Group 3 : Image Classification using Convolutional Neural Networks__\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZ6iKB3Xgpuk"
      },
      "source": [
        "## __Choosing the data set__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnEZ6_KOgpuk"
      },
      "source": [
        "We decided to work with a higher-resolution dataset to better understand how to handle images of varying sizes, bringing us closer to real-world scenarios. At the same time, we aim to focus on the learning process rather than grading outcomes, balancing both perspectives.\n",
        "\n",
        "While the lower-resolution CIFAR-10 dataset requires less computational power and shorter training times, dataset #2 demands a more complex architecture.\n",
        "\n",
        "As a team, we voted to prioritize gaining as much experience as possible with real-world challenges and chose dataset #2. We particularly anticipate learning new skills in data processing, coding, and CNN preprocessing, which will significantly enhance our expertise.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-UWfnTngpul"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.utils import make_grid\n",
        "from torchinfo import summary\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mrb20KGMtTFq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmxKL4mKgpum"
      },
      "source": [
        "### __Visualization:__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first image\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(x_train[0])\n",
        "plt.title(f\"Label: {y_train[0]}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wT_IAt0CuEHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0PuzNs4gpum"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a 10x10 grid\n",
        "plt.figure(figsize=(15, 15))  # Set the figure size for better visibility\n",
        "\n",
        "# Randomly select 100 indices\n",
        "random_indices = np.random.choice(len(x_train), 100, replace=False)\n",
        "\n",
        "for i, idx in enumerate(random_indices):\n",
        "    plt.subplot(10, 10, i + 1)  # Create a 10x10 grid\n",
        "    plt.imshow(x_train[idx], cmap=plt.get_cmap('gray'))  # Display a random image\n",
        "    plt.axis('off')  # Turn off axes for cleaner visualization\n",
        "\n",
        "plt.tight_layout()  # Adjust layout to avoid overlapping\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQfzeaeMgpum"
      },
      "source": [
        "### Get Range of values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXVW841Igpum"
      },
      "outputs": [],
      "source": [
        "print(\"Minimum value:\", np.min(x_train))  # Find the minimum value\n",
        "print(\"Maximum value:\", np.max(x_train))  # Find the maximum value\n",
        "print(\"Range:\", np.ptp(x_train))  # Range = max - min"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S9EEDyugpun"
      },
      "source": [
        "### Visualization Content:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Cq4lNy9gpun"
      },
      "outputs": [],
      "source": [
        "type(x_train)\n",
        "print(x_train)\n",
        "\n",
        "x_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB3I_UZbgpun"
      },
      "source": [
        "### Coverting Labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6yhNgpugpun"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Assuming your dataset loading looks like this\n",
        "# X_train, X_test, y_train, y_test = ... (load your dataset here)\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train_encoded = to_categorical(y_train, num_classes=10)  # Replace 10 with the number of classes\n",
        "y_test_encoded = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Optional: Print to verify\n",
        "print(y_train_encoded)\n",
        "print(y_test_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78N_IpW9gpun"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApaY-kytgpun"
      },
      "source": [
        "### Normalizing the Data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwIPrjAFgpun"
      },
      "outputs": [],
      "source": [
        "print(\"Minimum value:\", np.min(x_train))  # Find the minimum value\n",
        "print(\"Maximum value:\", np.max(x_train))  # Find the maximum value\n",
        "print(\"Range:\", np.ptp(x_train))  # Range = max - min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKzfr42Pgpun"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "\n",
        "# Assuming y_train and y_test are your target labels\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiT8CBFbgpuo"
      },
      "source": [
        "# __Defining the first model / taken from the Lab__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfWCHxh8HGhN"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()\n",
        "\n",
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape_value = (32, 32, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpT5HBiDgpuo"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation=\"relu\", input_shape = input_shape_value))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(num_classes, activation=\"softmax\"))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hn8UzPBZugVp"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.image import resize\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size_value = 512\n",
        "epochs_value = 1\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.002),  # Increased from 0.001\n",
        "               loss='categorical_crossentropy',\n",
        "               metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train_encoded, batch_size=batch_size_value, epochs=epochs_value, validation_data=(x_test, y_test_encoded))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsHmPCZygpuo"
      },
      "source": [
        "*   Plot the cross entropy loss curve and the accuracy curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gA41NbDgpuo"
      },
      "outputs": [],
      "source": [
        "print(history.history.keys())\n",
        "\n",
        "print(history.history['loss']) # returns the loss value at the end of each epoch\n",
        "print(history.history['accuracy']) # returns the accuracy at the end of each epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BY7rOSHkgpuo"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 5))  # Set figure size for better clarity\n",
        "\n",
        "# First subplot for loss\n",
        "plt.subplot(211)\n",
        "plt.title('Cross Entropy Loss Model #1')\n",
        "plt.plot(history.history['loss'], color='blue', label='Model #1')\n",
        "plt.xlabel('Epochs')  # Add x-label for consistency\n",
        "plt.ylabel('Loss')    # Add y-label\n",
        "plt.legend()\n",
        "\n",
        "# Second subplot for accuracy\n",
        "plt.subplot(212)\n",
        "plt.title('Classification Accuracy Model #1')\n",
        "plt.plot(history.history['accuracy'], color='blue', label='Model #1')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Adjust spacing between plots to prevent overlap\n",
        "plt.subplots_adjust(hspace=0.4)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])\n",
        "\n",
        "# Print results for both models\n",
        "print(\"\\033[1mFirst Architecture Approach:\\033[0m\")\n",
        "print(history.history.keys())\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1], \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVBRyWRFgpup"
      },
      "source": [
        "### Confuion Matrix to see the performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjFBkMMxgpup"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Predictions and ground truth\n",
        "predictions = model.predict(x_test)\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "gt = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(gt, predictions)\n",
        "\n",
        "# Create a heatmap using seaborn\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix Model #1')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CScS3cWfgpup"
      },
      "source": [
        "Scoring:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kzsi886ugpup"
      },
      "outputs": [],
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2mrWK5hSB_o"
      },
      "source": [
        "## Defining a #2 Model with deeper Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A80vLxW9FIek"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgca5dUNSFNc"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "model2 = keras.Sequential()\n",
        "\n",
        "model2.add(Conv2D(32, (3, 3), padding='same', activation=\"relu\", input_shape = input_shape_value))\n",
        "model2.add(Dropout(0.2))\n",
        "\n",
        "model2.add(Conv2D(32, (3, 3), padding='same', activation=\"relu\"))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model2.add(Dropout(0.3))\n",
        "\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(128, activation='relu'))\n",
        "model2.add(Dropout(0.5))\n",
        "model2.add(Dense(num_classes, activation=\"softmax\"))\n",
        "\n",
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc2qtU0mUvVA"
      },
      "outputs": [],
      "source": [
        "batch_size_value = 512\n",
        "epochs_value = 1\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',  # Stop when accuracy stops improving\n",
        "    patience=10,              # Allow 5 epochs without improvement\n",
        "    restore_best_weights=True\n",
        ")\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',     # Monitor validation loss\n",
        "    factor=0.5,             # Reduce learning rate by half\n",
        "    patience=3,             # Wait 3 epochs before reducing the rate\n",
        "    min_lr=1e-6             # Set a minimum learning rate\n",
        ")\n",
        "\n",
        "model2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.002),  # Increased from 0.001\n",
        "               loss='categorical_crossentropy',\n",
        "               metrics=['accuracy'],\n",
        "               )\n",
        "\n",
        "history2 = model2.fit(\n",
        "      x_train, y_train_encoded,\n",
        "      batch_size=batch_size_value,\n",
        "      epochs=epochs_value,\n",
        "      validation_data=(x_test, y_test_encoded),\n",
        "      callbacks=[reduce_lr, early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8OSHAf5SJPr"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 5))  # Set figure size for better clarity\n",
        "\n",
        "# First subplot for loss\n",
        "plt.subplot(211)\n",
        "plt.title('Cross Entropy Loss Model #1 & #2')\n",
        "plt.plot(history.history['loss'], color='blue', label='Model #1')\n",
        "plt.plot(history2.history['loss'], color='green', label='Model #2')\n",
        "plt.xlabel('Epochs')  # Add x-label for consistency\n",
        "plt.ylabel('Loss')    # Add y-label\n",
        "plt.legend()\n",
        "\n",
        "# Second subplot for accuracy\n",
        "plt.subplot(212)\n",
        "plt.title('Classification Accuracy Model #1 & #2')\n",
        "plt.plot(history.history['accuracy'], color='blue', label='Model #1')\n",
        "plt.plot(history2.history['accuracy'], color='green', label='Model #2')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Adjust spacing between plots to prevent overlap\n",
        "plt.subplots_adjust(hspace=0.4)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Print results for both models\n",
        "print(\"\\033[1mFirst Architecture Approach:\\033[0m\")\n",
        "print(history.history.keys())\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1], \"\\n\")\n",
        "\n",
        "print(\"\\033[1mDeeper Architecture:\\033[0m\")\n",
        "print(history2.history.keys())\n",
        "score2 = model2.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score2[0])\n",
        "print(\"Test accuracy:\", score2[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GjVHjTJgpuq"
      },
      "source": [
        "### Confusion Matrix to see the performance better"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DObaoxhaSMUg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define class names\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Predictions and ground truth for model #1\n",
        "predictions1 = model.predict(x_test)\n",
        "predictions1 = np.argmax(predictions1, axis=1)\n",
        "gt1 = np.argmax(y_test, axis=1)\n",
        "cm1 = confusion_matrix(gt1, predictions1)\n",
        "\n",
        "# Predictions and ground truth for model #2\n",
        "predictions2 = model2.predict(x_test)\n",
        "predictions2 = np.argmax(predictions2, axis=1)\n",
        "gt2 = np.argmax(y_test, axis=1)\n",
        "cm2 = confusion_matrix(gt2, predictions2)\n",
        "\n",
        "# Create subplots\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns\n",
        "\n",
        "# Confusion matrix for Model #1\n",
        "sns.heatmap(cm1, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=axs[0])\n",
        "axs[0].set_title('Confusion Matrix Model #1')\n",
        "axs[0].set_xlabel('Predicted Labels')\n",
        "axs[0].set_ylabel('True Labels')\n",
        "\n",
        "# Confusion matrix for Model #2\n",
        "sns.heatmap(cm2, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=axs[1])\n",
        "axs[1].set_title('Confusion Matrix Model #2')\n",
        "axs[1].set_xlabel('Predicted Labels')\n",
        "axs[1].set_ylabel('True Labels')\n",
        "\n",
        "# Adjust layout and show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4WX3_uLSN5I"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)  # Replace X_test and y_test with your data\n",
        "loss2, accuracy2 = model2.evaluate(x_test, y_test, verbose=0)  # Replace X_test and y_test with your data\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Test Accuracy Model #1: {accuracy * 100:.2f}%\")\n",
        "print(f\"Test Accuracy Model #2: {accuracy2 * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rFa3wjpgpuq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dySqfA6PVBjQ"
      },
      "source": [
        "## Defining a #3 Model with deeper Network / more complex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm35siILFNT0"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oH4lDVBuVA_Q"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "\n",
        "\n",
        "# Define input shape for the VGG-like architecture\n",
        "input_shape_value3 = (64, 64, 3)  # Input as per VGG guidelines\n",
        "num_classes = 10  # Number of classes for classification\n",
        "\n",
        "model3 = keras.Sequential()\n",
        "\n",
        "# Block 1\n",
        "model3.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape_value3))\n",
        "model3.add(BatchNormalization())  # Add Batch Normalization\n",
        "model3.add(Dropout(0.2))  # Moderate dropout after first conv block\n",
        "model3.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Block 2\n",
        "model3.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.5))  # Slightly higher dropout for deeper layers\n",
        "\n",
        "# Block 3\n",
        "model3.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model3.add(Dropout(0.5))  # Higher dropout for even deeper layers\n",
        "\n",
        "\n",
        "model3.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model3.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model3.add(Dropout(0.5))  # Higher dropout for even deeper layers\n",
        "\n",
        "\n",
        "# Replace Flatten with GAP\n",
        "model3.add(GlobalAveragePooling2D())  # GAP for dimensionality reduction\n",
        "model3.add(Dense(128, activation='relu'))  # Dense layer follows GAP\n",
        "model3.add(Dropout(0.5))  # Dropout for regularization\n",
        "model3.add(Dense(num_classes, activation='softmax'))  # Output layer\n",
        "\n",
        "model3.summary()  # Print the architecture summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJTW7G5Kgpur"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.image import resize\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Resize x_train and x_test\n",
        "x_train_resized = tf.image.resize(x_train, (64, 64), method=tf.image.ResizeMethod.BICUBIC)\n",
        "x_test_resized = tf.image.resize(x_test, (64, 64), method=tf.image.ResizeMethod.BICUBIC)\n",
        "y_train_resized = tf.image.resize(x_train, (64, 64), method=tf.image.ResizeMethod.BICUBIC)\n",
        "y_test_resized = tf.image.resize(x_test, (64, 64), method=tf.image.ResizeMethod.BICUBIC)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first image\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(x_train_resized[0])\n",
        "plt.title(f\"Label: {y_train_resized[0]}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w0PvOylAyVpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4elnDWnjEbmO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66e9c2c7-fc3d-40b5-8f5c-d588e7043ba5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Class Weights: {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0, 7: 1.0, 8: 1.0, 9: 1.0}\n",
            "Epoch 1/150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 50ms/step - accuracy: 0.2961 - loss: 1.9246 - val_accuracy: 0.3208 - val_loss: 2.1453 - learning_rate: 0.0010\n",
            "Epoch 2/150\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 42ms/step - accuracy: 0.4695 - loss: 1.4789 - val_accuracy: 0.5636 - val_loss: 1.1958 - learning_rate: 0.0010\n",
            "Epoch 3/150\n",
            "\u001b[1m1501/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.5286 - loss: 1.3225"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "batch_size_value = 512\n",
        "epochs_value = 150\n",
        "\n",
        "# Advanced Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "datagen.fit(x_train_resized)\n",
        "train_generator = datagen.flow(x_train_resized, y_train, batch_size=32)\n",
        "\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',  # Stop when accuracy stops improving\n",
        "    patience=10,              # Allow 5 epochs without improvement\n",
        "    restore_best_weights=True\n",
        ")\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',     # Monitor validation loss\n",
        "    factor=0.5,             # Reduce learning rate by half\n",
        "    patience=5,             # Wait 3 epochs before reducing the rate\n",
        "    min_lr=1e-7             # Set a minimum learning rate\n",
        ")\n",
        "\n",
        "# optimizing class cats\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    \"balanced\", classes=np.unique(y_train.argmax(axis=1)), y=y_train.argmax(axis=1)\n",
        ")\n",
        "# class_weights_dict[cat_class_index] *= 0.8  # Decrease the weight for \"cat\"\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "print(\"Final Class Weights:\", class_weights_dict)\n",
        "\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.AdamW(learning_rate=0.001)\n",
        "\n",
        "# Compile and Train Model with Scheduler\n",
        "model3.compile(\n",
        "    optimizer=optimizer,  # Initial LR\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history3 = model3.fit(\n",
        "    train_generator,\n",
        "    epochs= epochs_value,  # Use higher max epochs, but early stopping and scheduler will manage it\n",
        "    batch_size=batch_size_value,\n",
        "    validation_data=(x_test_resized, y_test),\n",
        "    callbacks=[reduce_lr, early_stopping],\n",
        "    class_weight=class_weights_dict\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19BMPCWfgpur"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 10))  # Set figure size for better clarity\n",
        "\n",
        "# First subplot for loss\n",
        "plt.subplot(211)\n",
        "plt.title('Cross Entropy Loss - Model #1, #2 & #3')\n",
        "plt.plot(history.history['loss'], color='blue', label='Model #1')\n",
        "plt.plot(history2.history['loss'], color='green', label='Model #2')\n",
        "plt.plot(history3.history['loss'], color='red', label='Model #3')\n",
        "plt.xlabel('Epochs')  # Add x-label for consistency\n",
        "plt.ylabel('Loss')    # Add y-label\n",
        "plt.legend()\n",
        "\n",
        "# Second subplot for accuracy\n",
        "plt.subplot(212)\n",
        "plt.title('Classification Accuracy - Model #1, #2 & #3')\n",
        "plt.plot(history.history['accuracy'], color='blue', label='Model #1')\n",
        "plt.plot(history2.history['accuracy'], color='green', label='Model #2')\n",
        "plt.plot(history3.history['accuracy'], color='red', label='Model #3')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Adjust spacing between plots to prevent overlap\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Print results for all models\n",
        "print(\"\\033[1mModel #1 Architecture:\\033[0m\")\n",
        "print(history.history.keys())\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1], '\\n')\n",
        "\n",
        "print(\"\\033[1mModel #2 Architecture:\\033[0m\")\n",
        "print(history2.history.keys())\n",
        "print(\"Test loss:\", score2[0])\n",
        "print(\"Test accuracy:\", score2[1], '\\n')\n",
        "\n",
        "print(\"\\033[1mModel #3 Architecture:\\033[0m\")\n",
        "print(history3.history.keys())\n",
        "score3 = model3.evaluate(x_test_resized, y_test, verbose=0)\n",
        "print(\"Test loss:\", score3[0])\n",
        "print(\"Test accuracy:\", score3[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc7c2-1Sgpur"
      },
      "source": [
        "### Confusion Matrix to see the performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9-2nAHxgpur"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define class names\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Predictions and ground truth for model #1\n",
        "predictions1 = model.predict(x_test)\n",
        "predictions1 = np.argmax(predictions1, axis=1)\n",
        "gt1 = np.argmax(y_test, axis=1)\n",
        "cm1 = confusion_matrix(gt1, predictions1)\n",
        "\n",
        "# Predictions and ground truth for model #2\n",
        "predictions2 = model2.predict(x_test)\n",
        "predictions2 = np.argmax(predictions2, axis=1)\n",
        "gt2 = np.argmax(y_test, axis=1)\n",
        "cm2 = confusion_matrix(gt2, predictions2)\n",
        "\n",
        "# Before making predictions, resize x_test to match the training size\n",
        "x_test_resized = tf.image.resize(x_test, (64, 64), method=tf.image.ResizeMethod.BICUBIC) # Assuming your model was trained on 128x128 images\n",
        "\n",
        "# Predictions and ground truth for model #3\n",
        "predictions3 = model3.predict(x_test_resized)  # Use the resized images for prediction\n",
        "predictions3 = np.argmax(predictions3, axis=1)\n",
        "gt3 = np.argmax(y_test, axis=1)\n",
        "cm3 = confusion_matrix(gt3, predictions3)\n",
        "\n",
        "# Create subplots for all three models\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 5))  # 1 row, 3 columns\n",
        "\n",
        "# Confusion matrix for Model #1\n",
        "sns.heatmap(cm1, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=axs[0])\n",
        "axs[0].set_title('Confusion Matrix Model #1')\n",
        "axs[0].set_xlabel('Predicted Labels')\n",
        "axs[0].set_ylabel('True Labels')\n",
        "\n",
        "# Confusion matrix for Model #2\n",
        "sns.heatmap(cm2, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=axs[1])\n",
        "axs[1].set_title('Confusion Matrix Model #2')\n",
        "axs[1].set_xlabel('Predicted Labels')\n",
        "axs[1].set_ylabel('True Labels')\n",
        "\n",
        "# Confusion matrix for Model #3\n",
        "sns.heatmap(cm3, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=axs[2])\n",
        "axs[2].set_title('Confusion Matrix Model #3')\n",
        "axs[2].set_xlabel('Predicted Labels')\n",
        "axs[2].set_ylabel('True Labels')\n",
        "\n",
        "# Adjust layout to ensure proper spacing\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEme75wygpus"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)  # Replace X_test and y_test with your data\n",
        "loss2, accuracy2 = model2.evaluate(x_test, y_test, verbose=0)  # Replace X_test and y_test with your data\n",
        "loss2, accuracy3 = model3.evaluate(x_test_resized, y_test, verbose=0)  # Replace X_test and y_test with your data\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Test Accuracy Model #1: {accuracy * 100:.2f}%\")\n",
        "print(f\"Test Accuracy Model #2: {accuracy2 * 100:.2f}%\")\n",
        "print(f\"Test Accuracy Model #3: {accuracy3 * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Plot the loss curve\n",
        "plt.figure()\n",
        "plt.plot(history3.history['loss'], label='Training Loss')\n",
        "plt.plot(history3.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Cross-Entropy Loss Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot the accuracy curve\n",
        "plt.figure()\n",
        "plt.plot(history3.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history3.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2Wbl9FvLFoFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o13pFDnbrxRa"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "print(\"Report of Model #3\")# Assuming `model3`, `x_test_resized`, and `class_names` are defined\n",
        "# Get the predicted probabilities\n",
        "predictions3_probs = model3.predict(x_test_resized)\n",
        "\n",
        "# Get the predicted classes from probabilities\n",
        "predicted_classes = np.argmax(predictions3_probs, axis=1)\n",
        "\n",
        "# Generate the classification report\n",
        "# print(classification_report(y_test.argmax(axis=1), predicted_classes, target_names=class_names))\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate the classification report as a dictionary\n",
        "report = classification_report(y_test.argmax(axis=1), predicted_classes, target_names=class_names, output_dict=True)\n",
        "\n",
        "# Convert the report dictionary to a pandas DataFrame for better formatting\n",
        "report_df = pd.DataFrame(report).transpose()\n",
        "\n",
        "# Display the classification report as a heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(report_df.iloc[:-1, :-1], annot=True, cmap='Blues', fmt='.2f', cbar=False)  # Exclude accuracy row\n",
        "plt.title(\"Classification Report Heatmap\")\n",
        "plt.xlabel(\"Metrics\")\n",
        "plt.ylabel(\"Classes\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linkcode\n",
        "y_pred_resnet=model_2.predict_classes(x_test)\n",
        "y_true=np.argmax(y_test,axis=1)\n",
        "\n",
        "#Compute the confusion matrix\n",
        "confusion_mtx=confusion_matrix(y_true,y_pred_resnet)"
      ],
      "metadata": {
        "id": "_LjvsJB8jAdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linkcode\n",
        "# Plot non-normalized confusion matrix\n",
        "plot_confusion_matrix(y_true, y_pred_resnet, classes=class_names,\n",
        "                      title='Confusion matrix, without normalization')"
      ],
      "metadata": {
        "id": "SyCKwSgWjD7I"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}